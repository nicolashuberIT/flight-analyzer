{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "This analysis tool reads .igc files from an indicated directory, filters the data using the `DataAnalyzer` helper class and then analyzes the speed data by applying the `SpeedAnalyzer` algorithm.\n",
    "\n",
    "Inputs:\n",
    "- INPUT_DIRECTORY: path to the directory containing .igc files\n",
    "- OUTPUT_DIRECTORY: output data will be exported to this directory\n",
    "- FILE_EXTENSION: set to `.igc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "# AI content (GitHub Copilot, 02/07/2024), verified and adapted by Nicolas Huber.\n",
    "src_directory: str = os.path.join(os.getcwd(), \"..\")\n",
    "sys.path.append(src_directory)\n",
    "\n",
    "import constants as constants\n",
    "import helpers.file_processor as file_processor\n",
    "import helpers.data_visualizer as data_visualizer\n",
    "import algorithms.speed_analyzer as speed_analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIRECTORY: str = \"/Users/nicolas/Dropbox/3_Bildung/2_KZO/Stichwortverzeichnis/0 Maturarbeit/6 Wettbewerbe/1_SJf/3_Coaching/2_Projekt/data-analysis/1_tracklogs/2_analysis-data/1_raw/2_glide\"\n",
    "OUTPUT_DIRECTORY: str = \"/Users/nicolas/Downloads/\" # end with /\n",
    "FILE_EXTENSION: str = \".igc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_processor = file_processor.FileProcessor()\n",
    "speed_analyzer = speed_analyzer.SpeedAnalyzer()\n",
    "data_visualizer = data_visualizer.DataVisualizer()\n",
    "\n",
    "theoretical_reference: pd.DataFrame = pd.read_csv(f\"{constants.THEORETICAL_REFERENCE_PATH}\")\n",
    "original_reference: pd.DataFrame = pd.read_csv(f\"{constants.ORIGINAL_REFERENCE_PATH}\")\n",
    "\n",
    "timestamp: str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths: List[str] = file_processor.get_file_paths(path=INPUT_DIRECTORY, file_extension=FILE_EXTENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Importing files for the following conditions:\")\n",
    "print(f\"--> Directory: {INPUT_DIRECTORY}\")\n",
    "print(f\"--> File extension: {FILE_EXTENSION}\")\n",
    "print(f\"--> Theoretical polar: {constants.THEORETICAL_REFERENCE_PATH.split(\"/\")[-1]}\")\n",
    "print(f\"--> Original reference: {constants.ORIGINAL_REFERENCE_PATH.split(\"/\")[-1]}\")\n",
    "print()\n",
    "print(\"Files:\")\n",
    "print(f\"--> Found {len(file_paths)} files.\")\n",
    "print(f\"--> The processing is initiated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw: pd.DataFrame = speed_analyzer.process_raw_data(file_paths=file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theoretical_reference_filtered: pd.DataFrame = speed_analyzer.filter_raw_data(data=theoretical_reference, reference=True)\n",
    "original_reference_filtered: pd.DataFrame = speed_analyzer.filter_raw_data(data=original_reference, reference=True)\n",
    "data_raw_filtered: pd.DataFrame = speed_analyzer.filter_raw_data(data=data_raw)\n",
    "\n",
    "print(f\"Filtered data:\")\n",
    "print(f\"--> Data points for filtered theoretical reference: {len(theoretical_reference_filtered)} (lost {len(theoretical_reference) - len(theoretical_reference_filtered)})\")\n",
    "print(f\"--> Data points for filtered original reference: {len(original_reference_filtered)} (lost {len(original_reference) - len(original_reference_filtered)})\")\n",
    "print(f\"--> Data points for filtered tracklogs: {len(data_raw_filtered)} (lost {len(data_raw) - len(data_raw_filtered)})\")\n",
    "print()\n",
    "print(f\"Please note:\")\n",
    "print(f\"--> Why are so many raw data points lost? This is due to the fact that the system filters out all data point that are not on a straight line. This is done to ensure that the data is as accurate as possible.\")\n",
    "\n",
    "smoothed_data_raw: pd.DataFrame = speed_analyzer.savgol_filter(data=data_raw_filtered)\n",
    "smoothed_data_grouped: pd.DataFrame = speed_analyzer.group_data(data=smoothed_data_raw)\n",
    "\n",
    "print(f\"Data smoothing:\")\n",
    "print(f\"--> During smoothing, the raw data points were reduced from {len(data_raw_filtered)} to {len(smoothed_data_raw)} (lost {len(data_raw_filtered) - len(smoothed_data_raw)}).\")\n",
    "print(f\"--> During grouping, the smoothed data points were reduced from {len(smoothed_data_raw)} to {len(smoothed_data_grouped)} (lost {len(smoothed_data_raw) - len(smoothed_data_grouped)}).\")\n",
    "print()\n",
    "print(f\"Export data to CSV:\")\n",
    "print(f\"--> Exporting smoothed data to {timestamp}_SJf_smoothed-experimental-speed-data_nicolas-huber.csv.\")\n",
    "print(f\"--> Exporting grouped data to {timestamp}_SJf_experimental-speed-data_nicolas-huber.csv.\")\n",
    "\n",
    "speed_analyzer.export_to_csv(data=smoothed_data_raw, file_path=f\"{OUTPUT_DIRECTORY}/{timestamp}_SJf_smoothed-experimental-speed-data_nicolas-huber.csv\")\n",
    "speed_analyzer.export_to_csv(data=smoothed_data_grouped, file_path=f\"{OUTPUT_DIRECTORY}/{timestamp}_SJf_grouped-experimental-speed-data_nicolas-huber.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data visualization:\")\n",
    "print(f\"--> For reference, the theoretical and experimental speed data is visualized.\")\n",
    "print(f\"--> To compare a older version of this algorithm, please refer to the following two plots:\")\n",
    "print()\n",
    "\n",
    "data_visualizer.visualize_raw_speed_data(experimental_data=original_reference_filtered, theoretical_data=theoretical_reference_filtered, std_error=0.2, title=\"Experimentelle Geschwindigkeistdaten (original)\")\n",
    "data_visualizer.visualize_raw_speed_data(experimental_data=smoothed_data_grouped, theoretical_data=theoretical_reference_filtered, std_error=0.05, title=\"Experimentelle Geschwindigkeistdaten (optimiert)\")\n",
    "\n",
    "print(f\"--> To visualize the quality increase of the data, the following two plots are shown:\")\n",
    "\n",
    "deviation_original: Tuple[float, float, float, float] = data_visualizer.visualize_speed_deviation(experimental_data=original_reference_filtered, theoretical_data=theoretical_reference_filtered, speed_analyzer=speed_analyzer, title=\"Abweichung der experimentellen Geschwindigkeitsdaten (original)\")\n",
    "deviation_optimized: Tuple[float, float, float, float] = data_visualizer.visualize_speed_deviation(experimental_data=smoothed_data_grouped, theoretical_data=theoretical_reference_filtered, speed_analyzer=speed_analyzer, title=\"Abweichung der experimentellen Geschwindigkeitsdaten (optimiert)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_original: float = speed_analyzer.score_stats(deviation_original)\n",
    "score_optimized: float = speed_analyzer.score_stats(deviation_optimized)\n",
    "\n",
    "print(f\"Report:\")\n",
    "speed_analyzer.print_report(score_original=score_original, score_optimized=score_optimized, deviation_original=deviation_original, deviation_optimized=deviation_optimized, datasets=[original_reference, original_reference_filtered, data_raw, smoothed_data_grouped])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"@ Author {constants.AUTHOR}\")\n",
    "print(f\"@ Author Email {constants.AUTHOR_EMAIL}\")\n",
    "print(f\"@ Author URL {constants.AUTHOR_URL}\")\n",
    "print(f\"@ GitHub URL {constants.GITHUB_URL}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
